{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlcIEIsGAYEq"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets \\\n",
        "                huggingface \\\n",
        "                jiwer \\\n",
        "                peft \\\n",
        "                transformers \\\n",
        "                torchaudio \\\n",
        "                torch \\\n",
        "                tqdm\n",
        "!pip install -U -q datasets\n",
        "!pip install -U -q bitsandbytes\n",
        "\n",
        "\n",
        "from datasets import Audio, DatasetDict, load_dataset\n",
        "from jiwer import wer, cer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import WhisperFeatureExtractor, \\\n",
        "                         WhisperForConditionalGeneration, \\\n",
        "                         WhisperProcessor, \\\n",
        "                         WhisperTokenizer\n",
        "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "from typing import Any\n",
        "import os\n",
        "\n",
        "base_model_name = \"openai/whisper-small\"\n",
        "language = \"romanian\"\n",
        "task = \"transcribe\"\n",
        "language_prefix = \"ro\"\n",
        "org = \"victors3136\"\n",
        "\n",
        "extractor = WhisperFeatureExtractor.from_pretrained(base_model_name)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(base_model_name, language=language, task=task)\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "def compute_metric(prediction):\n",
        "    pred_ids = prediction.predictions\n",
        "    label_ids = prediction.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_strs = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_strs = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    return {\n",
        "        \"wer\": wer(label_strs, pred_strs),\n",
        "        \"cer\": cer(label_strs, pred_strs)\n",
        "    }\n",
        "\n",
        "@dataclass\n",
        "class Collator:\n",
        "    processor: Any\n",
        "    def __call__(self, features):\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(labels_features, return_tensors=\"pt\")\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "def prepare(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_features\"] = extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "def train_model(it: str, sp: str):\n",
        "    dataset_name = f\"victors3136/dataset-5k-{it}it-{sp}sp\"\n",
        "    model_name = f\"whisper-model-small-ro-finetune-5k-{it}-{sp}\"\n",
        "    model_repo = f\"{org}/{model_name}\"\n",
        "\n",
        "    dataset = DatasetDict()\n",
        "    dataset[\"train\"] = load_dataset(dataset_name, split=\"train\").shuffle(seed=42)\n",
        "    dataset[\"validation\"] = load_dataset(dataset_name, split=\"val\")\n",
        "    dataset[\"test\"] = load_dataset(dataset_name, split=\"test\")\n",
        "\n",
        "    processor = WhisperProcessor.from_pretrained(base_model_name, language=language, task=task)\n",
        "    processor.tokenizer.set_prefix_tokens(language=language, task=task)\n",
        "\n",
        "    dataset = dataset \\\n",
        "                .cast_column(\"audio\", Audio(sampling_rate=16_000)) \\\n",
        "                .map(prepare, remove_columns=dataset[\"train\"].column_names, num_proc=4)\n",
        "\n",
        "    collator = Collator(processor=processor)\n",
        "\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(base_model_name, device_map=\"auto\")\n",
        "    model.config.forced_decoder_ids = None\n",
        "    model.config.suppress_tokens = []\n",
        "\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=model_repo,\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        warmup_steps=30,\n",
        "        num_train_epochs=5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        logging_steps=50,\n",
        "        fp16=True,\n",
        "        per_device_eval_batch_size=16,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=225,\n",
        "        remove_unused_columns=False,\n",
        "        report_to=\"wandb\",\n",
        "        push_to_hub=False\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        args=training_args,\n",
        "        model=model,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metric,\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(model_repo)\n",
        "    processor.save_pretrained(model_repo)\n",
        "    trainer.push_to_hub(\"Training done\")\n",
        "\n",
        "\n",
        "\n",
        "params = [\n",
        "          (\"00\", \"00\"),\n",
        "          (\"05\", \"05\"),\n",
        "          (\"15\", \"15\"),\n",
        "          (\"25\", \"25\"),\n",
        "          (\"35\", \"35\"),\n",
        "          (\"05\", \"25\"),\n",
        "          (\"25\", \"05\"),\n",
        "          (\"15\", \"35\"),\n",
        "          (\"35\", \"15\"),\n",
        "          (\"00\", \"50\"),\n",
        "          (\"50\", \"00\"),\n",
        "          (\"50\", \"50\"),\n",
        "        ]\n",
        "for param_set in tqdm(params, \"Training models... \"):\n",
        "    train_model(*param_set)"
      ]
    }
  ]
}