from expose_deep_phonemizer_module import expose_dp
from Loader.cv_loader import Loader
import uuid

expose_dp()

if __name__ == "__main__":
    print("Initializing Loader...")
    loader = Loader(it_fraction=0.2, es_fraction=0.2)

    print("Loading dataset (small-scale test)...")
    cache_dir = str(uuid.uuid4())
    dataset = loader.load(romanian_sample_count=100, output_dir=cache_dir)

    print("\nVerifying dataset splits...")
    assert "train" in dataset and "val" in dataset and "test" in dataset, "Missing one or more splits!"
    
    print(f"Train size: {len(dataset['train'])}")
    print(f"Val size: {len(dataset['val'])}")
    print(f"Test size: {len(dataset['test'])}")

    for split in ["train", "val", "test"]:
        for sample in dataset[split].select(range(min(5, len(dataset[split])))):
            assert "audio" in sample and "sentence" in sample, f"Sample missing fields in {split}"
            assert isinstance(sample["sentence"], str), f"Sentence must be string in {split}"
            assert isinstance(sample["audio"], dict), f"Audio must be dict (decoded) in {split}"
            assert "array" in sample["audio"], f"Decoded audio missing 'array' field in {split}"
        print(f"[{split}] Sample structure validated.")

    print("\nAll checks passed. Loader works as expected!")


    print("\nRe-Loading dataset (checking if we can skip reprocessing)...")
    dataset = loader.load(romanian_sample_count=100, output_dir=cache_dir)

    print("\nLoading slightly larger dataset (checking if we can skip some reprocessing)...")
    dataset = loader.load(romanian_sample_count=200, output_dir=cache_dir)


    print("\nWe don't want to remove any already present data if we request less than is available :)")
    dataset = loader.load(romanian_sample_count=50, output_dir=cache_dir)
    dataset = loader.load(romanian_sample_count=100, output_dir=cache_dir)

    print("\nCleaning up test cache directory...")
    Loader.cleanup(cache_dir)
    print("Cache removed.")


    """ EXPECTED OUTPUT:
    Initializing Loader...
    Loading G2P for ita ... 
    Loaded G2P for ita! 
    Initializing P2G ... 
    Loading P2G ... 
    P2G initialized!
    Loading G2P for spa ... 
    Loaded G2P for spa! 
    Initializing P2G ... 
    Loading P2G ... 
    P2G initialized!
    Loading dataset (small-scale test)...
    Streaming Romanian data...
    Reading metadata...: 5187it [00:00, 8414.68it/s]
    Collecting Romanian samples: 99it [00:02, 35.21it/s]
    Loading Italian data...
    [it] Need 16 more samples (have 0).
    [it] Streaming 16 new samples...
    Reading metadata...: 152609it [00:04, 34429.53it/s]
    Filtering it: 15it [00:05,  2.78it/s] 40693.67it/s]
    [it] Phonetically converting...
    Converting it: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  3.06it/s]
    Saving to disk ...
    Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 2707.75 examples/s]
    [it] Updated dataset saved with 16 samples.
    Loading Spanish data...
    [es] Need 16 more samples (have 0).
    [es] Streaming 16 new samples...
    Reading metadata...: 230467it [00:08, 27252.83it/s]
    Filtering es: 15it [00:09,  1.55it/s] 29936.32it/s]
    [es] Phonetically converting...
    Converting es: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  2.69it/s]
    Saving to disk ...
    Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 3183.53 examples/s]
    [es] Updated dataset saved with 16 samples.
    Shuffling training data set...
    Final sizes — Train: 112, Val: 10, Test: 10

    Verifying dataset splits...
    Train size: 112
    Val size: 10
    Test size: 10
    [train] Sample structure validated.
    [val] Sample structure validated.
    [test] Sample structure validated.

    All checks passed. Loader works as expected!

    Re-Loading dataset (checking if we can skip reprocessing)...
    Streaming Romanian data...
    Reading metadata...: 5187it [00:00, 14476.71it/s]
    Collecting Romanian samples: 99it [00:01, 78.69it/s] 
    Loading Italian data...
    [it] Loading existing dataset from disk...
    [it] Already have 16 samples.
    Loading Spanish data...
    [es] Loading existing dataset from disk...
    [es] Already have 16 samples.
    Shuffling training data set...
    Final sizes — Train: 112, Val: 10, Test: 10

    Loading slightly larger dataset (checking if we can skip some reprocessing)...
    Streaming Romanian data...
    Reading metadata...: 5187it [00:00, 17194.81it/s]
    Collecting Romanian samples: 199it [00:01, 102.03it/s]
    Loading Italian data...
    [it] Loading existing dataset from disk...
    [it] Need 16 more samples (have 16).
    [it] Streaming 16 new samples...
    Reading metadata...: 152609it [00:04, 34167.46it/s]
    Filtering it: 15it [00:05,  2.97it/s] 39769.86it/s]
    [it] Phonetically converting...
    Converting it: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  3.19it/s]
    Saving to disk ...
    Failed to save to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/it due to an older version of this dataset beign already present...
    Saving dataset temporarily to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/it_tmp_ed6d444f-106e-41ec-be71-c961ad02661c
    Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 2511.00 examples/s]
    Removing dataset from 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/it
    Moving new dataset to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/it_tmp_ed6d444f-106e-41ec-be71-c961ad02661c
    Loading Spanish data...
    [es] Loading existing dataset from disk...
    [es] Need 16 more samples (have 16).
    [es] Streaming 16 new samples...
    Reading metadata...: 230467it [00:08, 28713.47it/s]
    Filtering es: 15it [00:08,  1.74it/s] 30028.67it/s]
    [es] Phonetically converting...
    Converting es: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  2.89it/s]
    Saving to disk ...
    Failed to save to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/es due to an older version of this dataset beign already present...
    Saving dataset temporarily to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/es_tmp_610ab855-65bd-4709-b34d-0813705db1ef
    Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 3331.95 examples/s]
    Removing dataset from 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/es
    Moving new dataset to 6ffae50b-e5b0-442e-8e74-00bf9d3589e6/es_tmp_610ab855-65bd-4709-b34d-0813705db1ef
    Shuffling training data set...
    Final sizes — Train: 224, Val: 20, Test: 20

    We don't want to remove any already present data if we request less than is available :)
    Streaming Romanian data...
    Reading metadata...: 5187it [00:00, 17343.77it/s]
    Collecting Romanian samples: 49it [00:01, 32.03it/s]
    Loading Italian data...
    [it] Loading existing dataset from disk...
    [it] Already have 32 samples.
    Loading Spanish data...
    [es] Loading existing dataset from disk...
    [es] Already have 32 samples.
    Shuffling training data set...
    Final sizes — Train: 56, Val: 5, Test: 5
    Streaming Romanian data...
    Reading metadata...: 5187it [00:00, 10441.24it/s]
    Collecting Romanian samples: 99it [00:01, 67.50it/s] 
    Loading Italian data...
    [it] Loading existing dataset from disk...
    [it] Already have 32 samples.
    Loading Spanish data...
    [es] Loading existing dataset from disk...
    [es] Already have 32 samples.
    Shuffling training data set...
    Final sizes — Train: 112, Val: 10, Test: 10

    Cleaning up test cache directory...
    Cleaning 6ffae50b-e5b0-442e-8e74-00bf9d3589e6 up...
    Cache removed.
    """